{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b8659a2-0afa-4359-addd-a662b9150f4c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "CABECALHO"
    }
   },
   "outputs": [],
   "source": [
    "## NOTEBOOK DESENVOLVIDO PARA EXECUTAR TESTES UNITARIOS NO OBJETO CRIADO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3b5d274-135b-431e-9028-644887bc1afa",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "DROP TESTE"
    }
   },
   "outputs": [],
   "source": [
    "# %sql\n",
    "# drop table if exists dados_fake.asso_conta_movi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f135a513-1538-4f83-a338-fe42bccfa602",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "TESTE UNITARIOS"
    }
   },
   "outputs": [],
   "source": [
    "## EXECUTANDO TESTES UNITÁRIOS NO OBJETO\n",
    "testes_unitarios = None\n",
    "try:\n",
    "    ## VERIFICA SE A TABELA EXISTE\n",
    "    teste_status = \"TABELA NAO EXISTE\"\n",
    "    df_existe = spark.sql(\"\"\"\n",
    "        SELECT COUNT(*) AS tabela_existe\n",
    "        FROM information_schema.tables\n",
    "        WHERE table_schema = 'dados_fake' AND table_name = 'asso_conta_movi'\n",
    "    \"\"\")\n",
    "    if df_existe.collect()[0][\"tabela_existe\"] != 1:\n",
    "        raise Exception()\n",
    "    else:\n",
    "\n",
    "        ## VERIFICA SE ID_MOVIMENTACAO TEM VALORES NULOS\n",
    "        teste_status = \"ID_MOVIMENTOCAO TEM VALORES NULL\"\n",
    "        df_id_movimentacao_nulos = spark.sql(\"\"\"\n",
    "            SELECT COUNT(*) AS id_movimentacao_nulos\n",
    "            FROM dados_fake.asso_conta_movi\n",
    "            WHERE id_movimentacao IS NULL\n",
    "        \"\"\")\n",
    "        if df_id_movimentacao_nulos.collect()[0][\"id_movimentacao_nulos\"] > 0:\n",
    "            raise Exception()\n",
    "        else:\n",
    "\n",
    "            ## VERIFICA SE OS VALORES SÂO NUMERICOS\n",
    "            teste_status = \"VLR_TRANSACAO TEM VALORES NULL\"\n",
    "            df_vlr_transacao_nao_numerico = spark.sql(\"\"\"\n",
    "                SELECT COUNT(*) AS vlr_transacao_nao_numerico\n",
    "                FROM dados_fake.asso_conta_movi\n",
    "                WHERE TRY_CAST(vlr_transacao AS DOUBLE) IS NULL AND vlr_transacao IS NOT NULL\n",
    "            \"\"\")\n",
    "            if df_vlr_transacao_nao_numerico.collect()[0][\"vlr_transacao_nao_numerico\"] > 0:\n",
    "                spark.sql(\"SELECT raise_error('Existem valores não numéricos em vlr_transacao')\")\n",
    "            else:\n",
    "\n",
    "                ## VERIFICA SE EXISTE VALORES NEGATIVOS\n",
    "                teste_status = \"VLR_TRANSACAO TEM VALORES NEGATIVOS\"\n",
    "                df_vlr_transacao_negativos = spark.sql(\"\"\"\n",
    "                    SELECT COUNT(*) AS vlr_transacao_negativos\n",
    "                    FROM dados_fake.asso_conta_movi\n",
    "                    WHERE TRY_CAST(vlr_transacao AS DOUBLE) < 0\n",
    "                \"\"\")\n",
    "                if df_vlr_transacao_negativos.collect()[0][\"vlr_transacao_negativos\"] > 0:\n",
    "                    raise Exception()\n",
    "                else:\n",
    "\n",
    "                    ## VERIFICA SE A TABELA REGISTRA DATA FUTURA\n",
    "                    teste_status = \"TABELA COM DATA FUTURA\"\n",
    "                    df_transacoes_futuras = spark.sql(\"\"\"\n",
    "                        SELECT COUNT(*) AS transacoes_futuras\n",
    "                        FROM dados_fake.asso_conta_movi\n",
    "                        WHERE TO_DATE(dt_transacao) > CURRENT_DATE()\n",
    "                    \"\"\")\n",
    "                    if df_transacoes_futuras.collect()[0][\"transacoes_futuras\"] > 0:\n",
    "                        raise Exception()\n",
    "                    else:\n",
    "                        \n",
    "                        ## VERIFICA SE TODOS OS EMAILS TEM @\n",
    "                        teste_status = \"EMAIL SEM @\"\n",
    "                        df_emails_invalidos = spark.sql(\"\"\"\n",
    "                            SELECT COUNT(*) AS emails_invalidos\n",
    "                            FROM dados_fake.asso_conta_movi\n",
    "                            WHERE email NOT LIKE '%@%'\n",
    "                        \"\"\")\n",
    "                        if df_emails_invalidos.collect()[0][\"emails_invalidos\"] > 0:\n",
    "                            raise Exception()\n",
    "                        else:\n",
    "\n",
    "                            ## VERIFICA SE UNIQUE_ID É ÚNICO\n",
    "                            teste_status = \"CONSIDERANDO UNIQUE_ID A TABELA NAO TEM UNICIDADE\"\n",
    "                            df_unique_id_dup = spark.sql(\"\"\"\n",
    "                                SELECT COUNT(*) AS dup_count\n",
    "                                FROM (\n",
    "                                    SELECT UNIQUE_ID, COUNT(*) AS cnt\n",
    "                                    FROM dados_fake.asso_conta_movi\n",
    "                                    GROUP BY UNIQUE_ID\n",
    "                                    HAVING cnt > 1\n",
    "                                ) t\n",
    "                            \"\"\")\n",
    "                            if df_unique_id_dup.collect()[0][\"dup_count\"] > 0:\n",
    "                                raise Exception()\n",
    "                            else:\n",
    "                                testes_unitarios = \"OK\"\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    testes_unitarios = \"Error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f31be1c6-9ce7-4d11-857f-dacb7d967e9e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "EXPORT DADOS"
    }
   },
   "outputs": [],
   "source": [
    "## EXPORTANDO DADOS EM FORMATO DELTA E PARQUET\n",
    "if testes_unitarios == \"OK\":\n",
    "\n",
    "    ## LENDO A TABELA\n",
    "    df_export = spark.table(\"\"\"dados_fake.asso_conta_movi\"\"\")\n",
    "\n",
    "    ## EXPORT TIPO DELTA\n",
    "    df_export.write.mode(\"overwrite\").format(\"delta\").save(\"/Volumes/workspace/default/desafio_tecnico/export_data//asso_conta_movi.delta\")\n",
    "\n",
    "    ## EXPORT TIPO PARQUET\n",
    "    df_export.write.mode(\"overwrite\").format(\"parquet\").save(\"/Volumes/workspace/default/desafio_tecnico/export_data//asso_conta_movi.parquet\")\n",
    "\n",
    "    print(f\"Objeto exportado em formato DELTA E PARQUET'\")\n",
    "else:\n",
    "    print(f\"O objeto não foi aprovado nos testes unitarios: '{teste_status}'\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7978444455609165,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "dados_teste_unitarios",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
